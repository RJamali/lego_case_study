{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc443eb8-03e6-481c-939a-77a5ca69b0a2",
   "metadata": {},
   "source": [
    "# Case Study: Building a production-ready recommendation engine for a new customer\n",
    "\n",
    "**Author:** Ruhollah Jamali \n",
    "\n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook presents a complete solution for three key recommendation scenarios:\n",
    "1. **Top-Seller Products**\n",
    "2. **Frequently Bought Together (FreBoTo)**\n",
    "3. **Real-time Personalized Recommendations**\n",
    "\n",
    "\n",
    "### Dataset\n",
    "We're using the [Otto RecSys Challenge dataset](https://www.kaggle.com/datasets/otto/recsys-dataset), which contains ~12M e-commerce sessions with click, cart, and purchase events. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4c99d-e0fd-4801-bf67-79b190156761",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a197dd8-1527-4d6d-b9fb-dffaa29f2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import polars as pl\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import kaggle\n",
    "import zipfile\n",
    "import subprocess\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "# ML and similarity computation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz\n",
    "import pickle\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"C:\\Program Files\\Graphviz\\bin\"\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "# API\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# For persistence and model saving\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad564a47-1fc3-4bd1-89bd-d0928970e6d9",
   "metadata": {},
   "source": [
    "## Project setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6171a54c-1390-426a-a696-c8b533fc7bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure project directories\n",
    "BASE_DIR = Path.cwd().parent\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "OUTPUT_DIR = BASE_DIR / 'output'\n",
    "\n",
    "# Create dirs if they don't exist\n",
    "for directory in [DATA_DIR, MODELS_DIR, OUTPUT_DIR]:\n",
    "    directory.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d353379-4de8-44d4-9743-b112efedd50d",
   "metadata": {},
   "source": [
    "# Data loading and understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e941d4-9c00-4ffe-8e33-1be9da9132a3",
   "metadata": {},
   "source": [
    "## Download dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad240c2f-8e62-4028-8620-470260400ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists in data/ folder, skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset using the Kaggle API.\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "dataset_slug = \"otto/recsys-dataset\"\n",
    "target_filename = \"otto-recsys-train.jsonl\"\n",
    "target_path = DATA_DIR / target_filename\n",
    "\n",
    "# Set up environment variable if needed\n",
    "os.environ.setdefault(\"KAGGLE_CONFIG_DIR\", str(Path.home() / \".kaggle\"))\n",
    "\n",
    "# Download dataset file if not already present\n",
    "if not target_path.exists():\n",
    "    print(\"Downloading dataset from Kaggle...\")\n",
    "    subprocess.run([\n",
    "        \"kaggle\", \"datasets\", \"download\",\n",
    "        \"-d\", dataset_slug,\n",
    "        \"-f\", target_filename,\n",
    "        \"-p\", str(DATA_DIR)\n",
    "    ], check=True)\n",
    "    \n",
    "    # Unzipping the file using Python's zipfile\n",
    "    for file in DATA_DIR.glob(\"*.zip\"):\n",
    "        print('unzip')\n",
    "        print(f\"Extracting {file.name}...\")\n",
    "        with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(DATA_DIR)\n",
    "        file.unlink()  # Remove the zip file after extraction\n",
    "    \n",
    "    print(\"Download completed and file ready in data/ folder.\")\n",
    "else:\n",
    "    print(\"Dataset already exists in data/ folder, skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74610424-df96-476f-acfd-bb0e4b0e95dc",
   "metadata": {},
   "source": [
    "## Data conversion from JSONL to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66aa539a-7993-45d3-8c77-7128d9fc7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = DATA_DIR / \"otto-recsys-train.jsonl\"\n",
    "\n",
    "def convert_jsonl_to_parquet_streaming():\n",
    "    jsonl_path = DATA_DIR / 'otto-recsys-train.jsonl'\n",
    "    parquet_path = DATA_DIR / 'train_full.parquet'\n",
    "\n",
    "    print(\"Converting JSONL to Parquet\")\n",
    "\n",
    "    BATCH_SIZE = 50000 \n",
    "    batch = []\n",
    "    writer = None\n",
    "    schema = None\n",
    "    total_events = 0\n",
    "    total_lines = 0\n",
    "\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        for line in f:\n",
    "            total_lines += 1\n",
    "            record = json.loads(line)\n",
    "\n",
    "            # Flatten nested events\n",
    "            for event in record['events']:\n",
    "                batch.append({\n",
    "                    'session_id': record['session'],\n",
    "                    'item_id': event['aid'],\n",
    "                    'ts': event['ts'],\n",
    "                    'event_type': event['type']\n",
    "                })\n",
    "\n",
    "                # When batch is full, write to parquet\n",
    "                if len(batch) >= BATCH_SIZE:\n",
    "                    table = pa.Table.from_pylist(batch)\n",
    "\n",
    "                    if writer is None:\n",
    "                        # First batch: create writer with schema\n",
    "                        schema = table.schema\n",
    "                        writer = pq.ParquetWriter(parquet_path, schema)\n",
    "\n",
    "                    writer.write_table(table)\n",
    "                    total_events += len(batch)\n",
    "                    batch.clear()\n",
    "\n",
    "                    if total_lines % 10000 == 0:\n",
    "                        print(f\"  Processed {total_lines:,} sessions, {total_events:,} events...\")\n",
    "\n",
    "    # Write remaining batch\n",
    "    if batch:\n",
    "        table = pa.Table.from_pylist(batch)\n",
    "        if writer is None:\n",
    "            schema = table.schema\n",
    "            writer = pq.ParquetWriter(parquet_path, schema)\n",
    "        writer.write_table(table)\n",
    "        total_events += len(batch)\n",
    "        batch.clear()\n",
    "\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "    print(f\"Conversion complete! {total_events:,} events saved to Parquet\")\n",
    "    return parquet_path\n",
    "\n",
    "source_parquet = DATA_DIR / 'train_full.parquet'\n",
    "\n",
    "if not source_parquet.exists():\n",
    "    print(\"Source parquet not found, converting from JSONL...\")\n",
    "    convert_jsonl_to_parquet_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b964d-3faa-4b39-a59f-f3cb1baa9c07",
   "metadata": {},
   "source": [
    "## Load and inspect raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66807be4-bcfa-48cd-9b9f-9ae9541f6395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Parquet file: train_full.parquet\n",
      "[data] Row groups: 4,335, Total rows: 216,716,096\n",
      "[data] Columns: ['session_id', 'item_id', 'ts', 'event_type']\n"
     ]
    }
   ],
   "source": [
    "meta = pq.read_metadata(source_parquet)\n",
    "print(f\"[data] Parquet file: {source_parquet.name}\")\n",
    "print(f\"[data] Row groups: {meta.num_row_groups:,}, Total rows: {meta.num_rows:,}\")\n",
    "print(f\"[data] Columns: {[meta.schema.names[i] for i in range(meta.num_columns)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8664f5-9556-4500-812c-ed244b287576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Sampled 150,000 rows from 3 row groups (~0.07% of dataset).\n",
      "\n",
      "[data] Descriptive statistics on sample:\n",
      "shape: (9, 5)\n",
      "┌────────────┬────────────┬───────────────┬───────────┬────────────┐\n",
      "│ statistic  ┆ session_id ┆ item_id       ┆ ts        ┆ event_type │\n",
      "│ ---        ┆ ---        ┆ ---           ┆ ---       ┆ ---        │\n",
      "│ str        ┆ f64        ┆ f64           ┆ f64       ┆ str        │\n",
      "╞════════════╪════════════╪═══════════════╪═══════════╪════════════╡\n",
      "│ count      ┆ 150000.0   ┆ 150000.0      ┆ 150000.0  ┆ 150000     │\n",
      "│ null_count ┆ 0.0        ┆ 0.0           ┆ 0.0       ┆ 0          │\n",
      "│ mean       ┆ 946.858633 ┆ 927869.98524  ┆ 1.6602e12 ┆ null       │\n",
      "│ std        ┆ 570.758656 ┆ 535049.196347 ┆ 7.6404e8  ┆ null       │\n",
      "│ min        ┆ 0.0        ┆ 114.0         ┆ 1.6593e12 ┆ carts      │\n",
      "│ 25%        ┆ 474.0      ┆ 467667.0      ┆ 1.6594e12 ┆ null       │\n",
      "│ 50%        ┆ 939.0      ┆ 927947.0      ┆ 1.6600e12 ┆ null       │\n",
      "│ 75%        ┆ 1436.0     ┆ 1.390803e6    ┆ 1.6608e12 ┆ null       │\n",
      "│ max        ┆ 1949.0     ┆ 1.855597e6    ┆ 1.6617e12 ┆ orders     │\n",
      "└────────────┴────────────┴───────────────┴───────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "reader = pq.ParquetFile(source_parquet)\n",
    "\n",
    "# Choose a few row groups to sample (depends on available memory)\n",
    "n_groups = min(3, reader.num_row_groups)\n",
    "sample_tables = [reader.read_row_group(i) for i in range(n_groups)]\n",
    "sample_df = pl.concat([pl.from_arrow(tbl) for tbl in sample_tables])\n",
    "\n",
    "print(f\"[data] Sampled {len(sample_df):,} rows from {n_groups} row groups (~{100 * n_groups / reader.num_row_groups:.2f}% of dataset).\")\n",
    "\n",
    "# Compute simple descriptive stats\n",
    "print(\"\\n[data] Descriptive statistics on sample:\")\n",
    "print(sample_df.describe())\n",
    "\n",
    "# Optional: event distribution (categorical overview)\n",
    "if \"type\" in sample_df.columns:\n",
    "    print(\"\\n[data] Event type distribution:\")\n",
    "    print(sample_df[\"type\"].value_counts().to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf06f27-4760-4332-b222-048cf8cc6eba",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c925f62a-54d1-40a8-ac33-6b5e2cfd94b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data already exists at train_full_processed.parquet\n",
      "Preprocessed data is ready and it has: 216,716,096 records\n"
     ]
    }
   ],
   "source": [
    "# Load data directly from source parquet (keeps original columns)\n",
    "df_raw = pl.scan_parquet(source_parquet)\n",
    "\n",
    "# Check if processed version already exists\n",
    "processed_path = DATA_DIR / 'train_full_processed.parquet'\n",
    "\n",
    "if not processed_path.exists():\n",
    "    print(\"Preprocessing data and saving it on the disk...\")\n",
    "\n",
    "    # Build preprocessing pipeline\n",
    "    df_processed = (\n",
    "        df_raw\n",
    "        .with_columns([\n",
    "            pl.col(\"event_type\").cast(pl.Categorical),\n",
    "            pl.from_epoch(pl.col('ts'), time_unit='ms').alias('event_time')\n",
    "        ])\n",
    "        .select(['session_id', 'item_id', 'event_time', 'event_type'])\n",
    "    )\n",
    "\n",
    "    # Write directly to disk \n",
    "    print(\"Writing to parquet...\")\n",
    "    df_processed.sink_parquet(processed_path, compression='snappy', row_group_size=100000)\n",
    "    print(f\"Preprocessed data saved to {processed_path.name}\")\n",
    "else:\n",
    "    print(f\"Preprocessed data already exists at {processed_path.name}\")\n",
    "\n",
    "# Work with the processed data\n",
    "df_final = pl.scan_parquet(processed_path)\n",
    "\n",
    "# Verify the file is valid\n",
    "verification_count = df_final.select(pl.len()).collect().item()\n",
    "print(f\"Preprocessed data is ready and it has: {verification_count:,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064eb3d3-cd4f-410e-b372-33c4a303712e",
   "metadata": {},
   "source": [
    "## Load and inspect preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7727e683-045b-4db8-91b2-2d9f934a1f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event type Distribution:\n",
      "shape: (3, 2)\n",
      "┌────────────┬───────────┐\n",
      "│ event_type ┆ count     │\n",
      "│ ---        ┆ ---       │\n",
      "│ cat        ┆ u32       │\n",
      "╞════════════╪═══════════╡\n",
      "│ clicks     ┆ 194720954 │\n",
      "│ carts      ┆ 16896191  │\n",
      "│ orders     ┆ 5098951   │\n",
      "└────────────┴───────────┘\n",
      "\n",
      "Session analysis:\n",
      "shape: (1, 4)\n",
      "┌────────────┬───────────────┬────────────┬────────────┐\n",
      "│ avg_events ┆ median_events ┆ max_events ┆ min_events │\n",
      "│ ---        ┆ ---           ┆ ---        ┆ ---        │\n",
      "│ f64        ┆ f64           ┆ u32        ┆ u32        │\n",
      "╞════════════╪═══════════════╪════════════╪════════════╡\n",
      "│ 16.799985  ┆ 6.0           ┆ 500        ┆ 2          │\n",
      "└────────────┴───────────────┴────────────┴────────────┘\n",
      "\n",
      "Top 10 Most interacted items\n",
      "shape: (10, 2)\n",
      "┌─────────┬──────────────┐\n",
      "│ item_id ┆ interactions │\n",
      "│ ---     ┆ ---          │\n",
      "│ i64     ┆ u32          │\n",
      "╞═════════╪══════════════╡\n",
      "│ 1460571 ┆ 129004       │\n",
      "│ 485256  ┆ 126836       │\n",
      "│ 108125  ┆ 118524       │\n",
      "│ 29735   ┆ 113279       │\n",
      "│ 1733943 ┆ 105091       │\n",
      "│ 832192  ┆ 91325        │\n",
      "│ 184976  ┆ 90244        │\n",
      "│ 166037  ┆ 84657        │\n",
      "│ 554660  ┆ 80197        │\n",
      "│ 231487  ┆ 79872        │\n",
      "└─────────┴──────────────┘\n",
      "\n",
      "Dataset schema\n",
      "Columns: ['session_id', 'item_id', 'event_time', 'event_type']\n",
      "Data types: [Int64, Int64, Datetime(time_unit='ms', time_zone=None), Categorical]\n",
      "\n",
      "Sample records:\n",
      "shape: (5, 4)\n",
      "┌────────────┬─────────┬─────────────────────────┬────────────┐\n",
      "│ session_id ┆ item_id ┆ event_time              ┆ event_type │\n",
      "│ ---        ┆ ---     ┆ ---                     ┆ ---        │\n",
      "│ i64        ┆ i64     ┆ datetime[ms]            ┆ cat        │\n",
      "╞════════════╪═════════╪═════════════════════════╪════════════╡\n",
      "│ 0          ┆ 1517085 ┆ 2022-07-31 22:00:00.025 ┆ clicks     │\n",
      "│ 0          ┆ 1563459 ┆ 2022-07-31 22:01:44.511 ┆ clicks     │\n",
      "│ 0          ┆ 1309446 ┆ 2022-08-01 15:23:59.426 ┆ clicks     │\n",
      "│ 0          ┆ 16246   ┆ 2022-08-01 15:28:39.997 ┆ clicks     │\n",
      "│ 0          ┆ 1781822 ┆ 2022-08-01 15:31:11.344 ┆ clicks     │\n",
      "└────────────┴─────────┴─────────────────────────┴────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"Event type Distribution:\")\n",
    "event_counts = (\n",
    "    df_final\n",
    "    .group_by('event_type')\n",
    "    .agg(pl.len().alias('count'))\n",
    "    .sort('count', descending=True)\n",
    "    .collect()\n",
    ")\n",
    "print(event_counts)\n",
    "\n",
    "# Session length analysis\n",
    "print(\"\\nSession analysis:\")\n",
    "session_stats = (\n",
    "    df_final\n",
    "    .group_by('session_id')\n",
    "    .agg(pl.len().alias('events_per_session'))\n",
    "    .select([\n",
    "        pl.col('events_per_session').mean().alias('avg_events'),\n",
    "        pl.col('events_per_session').median().alias('median_events'),\n",
    "        pl.col('events_per_session').max().alias('max_events'),\n",
    "        pl.col('events_per_session').min().alias('min_events')\n",
    "    ])\n",
    "    .collect()\n",
    ")\n",
    "print(session_stats)\n",
    "\n",
    "# Item popularity overview\n",
    "print(\"\\nTop 10 Most interacted items\")\n",
    "top_items = (\n",
    "    df_final\n",
    "    .group_by('item_id')\n",
    "    .agg(pl.len().alias('interactions'))\n",
    "    .sort('interactions', descending=True)\n",
    "    .head(10)\n",
    "    .collect()\n",
    ")\n",
    "print(top_items)\n",
    "\n",
    "# Basic dataset info\n",
    "print(\"\\nDataset schema\")\n",
    "schema_sample = df_final.head(5).collect()\n",
    "print(f\"Columns: {schema_sample.columns}\")\n",
    "print(f\"Data types: {schema_sample.dtypes}\")\n",
    "print(\"\\nSample records:\")\n",
    "print(schema_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5afa2-4d05-42af-9661-fd882fda5f97",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a57b9cfc-b762-4638-b9fa-f24a0d64c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating aggregated features for recommendation models...\n",
      "Computing item popularity scores...\n",
      "Saving item statistics to item_popularity_stats.parquet...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating aggregated features for recommendation models...\")\n",
    "\n",
    "# Calculate item popularity metrics across all events\n",
    "print(\"Computing item popularity scores...\")\n",
    "\n",
    "item_stats = (\n",
    "    df_final\n",
    "    .group_by('item_id')\n",
    "    .agg([\n",
    "        pl.len().alias('total_interactions'),\n",
    "        pl.col('event_type').filter(pl.col('event_type') == 'clicks').len().alias('clicks'),\n",
    "        pl.col('event_type').filter(pl.col('event_type') == 'carts').len().alias('carts'),\n",
    "        pl.col('event_type').filter(pl.col('event_type') == 'orders').len().alias('orders'),\n",
    "    ])\n",
    "    .with_columns([\n",
    "        # Weighted popularity score: orders worth more than clicks\n",
    "        (pl.col('orders') * 3 + pl.col('carts') * 2 + pl.col('clicks')).alias('popularity_score')\n",
    "    ])\n",
    "    .sort('popularity_score', descending=True)\n",
    ")\n",
    "\n",
    "# Save item statistics for later use\n",
    "item_stats_path = OUTPUT_DIR / 'item_popularity_stats.parquet'\n",
    "print(f\"Saving item statistics to {item_stats_path.name}...\")\n",
    "item_stats.sink_parquet(item_stats_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45af98-7a98-4930-a5f1-1b9a5731dec3",
   "metadata": {},
   "source": [
    "# Applicaitons implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c9a21-6d09-4348-94d7-75b9e6a5bc1d",
   "metadata": {},
   "source": [
    "## Top-Seller Products Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329d60f6-e125-4b9b-b39c-69414d468ab1",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fde889f-1857-42de-9346-f75aff1a9469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding top seller products\n",
      "shape: (50, 5)\n",
      "┌─────────┬───────────────────┬────────┬───────┬────────┐\n",
      "│ item_id ┆ purchase_freqency ┆ orders ┆ carts ┆ clicks │\n",
      "│ ---     ┆ ---               ┆ ---    ┆ ---   ┆ ---    │\n",
      "│ i64     ┆ u32               ┆ u32    ┆ u32   ┆ u32    │\n",
      "╞═════════╪═══════════════════╪════════╪═══════╪════════╡\n",
      "│ 231487  ┆ 4485              ┆ 4485   ┆ 10393 ┆ 64994  │\n",
      "│ 166037  ┆ 3824              ┆ 3824   ┆ 13476 ┆ 67357  │\n",
      "│ 1733943 ┆ 3042              ┆ 3042   ┆ 10654 ┆ 91395  │\n",
      "│ 1445562 ┆ 2998              ┆ 2998   ┆ 5745  ┆ 12888  │\n",
      "│ 1022566 ┆ 2788              ┆ 2788   ┆ 9338  ┆ 51045  │\n",
      "│ …       ┆ …                 ┆ …      ┆ …     ┆ …      │\n",
      "│ 180910  ┆ 1529              ┆ 1529   ┆ 3553  ┆ 21758  │\n",
      "│ 1531805 ┆ 1464              ┆ 1464   ┆ 4435  ┆ 44571  │\n",
      "│ 898836  ┆ 1460              ┆ 1460   ┆ 2609  ┆ 14281  │\n",
      "│ 432989  ┆ 1443              ┆ 1443   ┆ 3036  ┆ 23846  │\n",
      "│ 1406660 ┆ 1437              ┆ 1437   ┆ 4405  ┆ 33640  │\n",
      "└─────────┴───────────────────┴────────┴───────┴────────┘\n",
      "\n",
      "\n",
      "Models saved: top_sellers_purchase.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"Finding top seller products\")\n",
    "\n",
    "# Load the precomputed item stats\n",
    "item_stats_df = pl.read_parquet(OUTPUT_DIR / 'item_popularity_stats.parquet')\n",
    "\n",
    "purchase_based = (item_stats_df.filter(pl.col('orders') > 0).with_columns([(pl.col('orders')).alias('purchase_freqency')])\n",
    "                    .sort('purchase_freqency', descending=True)\n",
    "                    .head(100))\n",
    "\n",
    "print(purchase_based.head(50).select(['item_id', 'purchase_freqency', 'orders', 'carts', 'clicks']))\n",
    "# Save model\n",
    "purchase_model_path = MODELS_DIR / 'top_sellers_purchase.parquet'\n",
    "purchase_based.write_parquet(purchase_model_path)\n",
    "print(f\"\\n\\nModels saved: {purchase_model_path.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0235961-533f-42f2-9234-401cd9415097",
   "metadata": {},
   "source": [
    "### API function for top-seller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c6a4fdc-20aa-4bad-9e35-018dc773b4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "API Test - Top 20 sellers: [231487, 166037, 1733943, 1445562, 1022566, 801774, 1629608, 756588, 332654, 1603001, 409620, 1257293, 1125638, 986164, 1083665, 450505, 544144, 1025795, 125278, 29735]\n",
      "\n",
      "\n",
      "Model metadata saved\n"
     ]
    }
   ],
   "source": [
    "# Create improved API function\n",
    "# This function retrieve top n selling products\n",
    "def get_top_sellers(n=10):\n",
    "    df = pl.read_parquet(MODELS_DIR / 'top_sellers_purchase.parquet')\n",
    "    return df.head(n)['item_id'].to_list()\n",
    "\n",
    "# Test API\n",
    "print(f\"\\n\\nAPI Test - Top 20 sellers: {get_top_sellers(20)}\")\n",
    "\n",
    "# Save model metadata\n",
    "model_metadata = {\n",
    "    'model_name': 'top_sellers',\n",
    "    'model_version': '1.0',\n",
    "    'created_at': pd.Timestamp.now().isoformat(),\n",
    "    'strategies': {\n",
    "        'purchase': {\n",
    "            'description': 'Items with purchases frequency',\n",
    "            'total_products': len(purchase_based)\n",
    "        },\n",
    "    },\n",
    "    'recommendation': 'Use purchase-based for homepage/main recommendations'\n",
    "}\n",
    "\n",
    "with open(MODELS_DIR / 'top_sellers_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(\"\\n\\nModel metadata saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d7a61-d692-4760-b943-8a352458c3c8",
   "metadata": {},
   "source": [
    "## Frequently bought together (FreBoTo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd58467-ba67-41ce-a528-c24f148f60ca",
   "metadata": {},
   "source": [
    "###  Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886e86b8-910d-4298-a111-f4bffc49b992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Frequently Bought Together Model\n",
      "Extracting co-occurrence patterns from purchase sessions...\n",
      "Filtering out rare items to improve model quality...\n",
      "Saving filtered order sessions to order_sessions.parquet...\n",
      "Order session data prepared\n"
     ]
    }
   ],
   "source": [
    "print(\"Building Frequently Bought Together Model\")\n",
    "\n",
    "# We need to find items that appear together in the same session while focusing on purchase events for true buying patterns\n",
    "\n",
    "print(\"Extracting co-occurrence patterns from purchase sessions...\")\n",
    "\n",
    "# Get sessions with orders only\n",
    "order_sessions = (df_final.filter(pl.col('event_type') == 'orders').select(['session_id', 'item_id']).unique())\n",
    "\n",
    "# Also we count sessions per item to filter out very rare items\n",
    "print(\"Filtering out rare items to improve model quality...\")\n",
    "\n",
    "item_session_counts = (order_sessions.group_by('item_id').agg(pl.len().alias('session_count'))\n",
    "                            .filter(pl.col('session_count') >= 3) )  # Item must appear in at least 3 sessions\n",
    "\n",
    "# Filter sessions to only include popular enough items\n",
    "filtered_sessions = (order_sessions.join(item_session_counts, on='item_id', how='inner')\n",
    "                        .select(['session_id', 'item_id']))\n",
    "\n",
    "# Save for co-occurrence computation\n",
    "sessions_path = OUTPUT_DIR / 'order_sessions.parquet'\n",
    "print(f\"Saving filtered order sessions to {sessions_path.name}...\")\n",
    "filtered_sessions.sink_parquet(sessions_path)\n",
    "\n",
    "print(\"Order session data prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3016ca5-b674-4b25-9c52-fe063709e588",
   "metadata": {},
   "source": [
    "### FreBoTo - Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f25ed453-aedd-46cb-add4-387e87014d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing item co-occurrence matrix...\n",
      "Total sessions with orders: 1,517,379\n",
      "Generating item pairs from sessions...\n",
      "Processed 500,000 sessions...\n",
      "Processed 1,000,000 sessions...\n",
      "Processed 1,500,000 sessions...\n",
      "\n",
      "Total item pairs generated: 10,891,476\n",
      "Counting co-occurrence frequencies...\n",
      "\n",
      "Unique item pairs with co-occurrences: 9,564,041\n",
      "\n",
      "Top 10 Most Frequently Bought Together:\n",
      "shape: (10, 3)\n",
      "┌─────────┬─────────┬────────────────────┐\n",
      "│ item_a  ┆ item_b  ┆ cooccurrence_count │\n",
      "│ ---     ┆ ---     ┆ ---                │\n",
      "│ i64     ┆ i64     ┆ i64                │\n",
      "╞═════════╪═════════╪════════════════════╡\n",
      "│ 125278  ┆ 1629608 ┆ 671                │\n",
      "│ 1302234 ┆ 1647157 ┆ 575                │\n",
      "│ 231487  ┆ 756588  ┆ 550                │\n",
      "│ 125278  ┆ 162064  ┆ 486                │\n",
      "│ 80222   ┆ 351335  ┆ 478                │\n",
      "│ 933686  ┆ 1446430 ┆ 458                │\n",
      "│ 821490  ┆ 1703316 ┆ 437                │\n",
      "│ 162064  ┆ 1629608 ┆ 431                │\n",
      "│ 576949  ┆ 1647157 ┆ 430                │\n",
      "│ 501077  ┆ 1199520 ┆ 422                │\n",
      "└─────────┴─────────┴────────────────────┘\n",
      "\n",
      "Co-occurrence matrix saved to item_cooccurrence.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing item co-occurrence matrix...\")\n",
    "\n",
    "# Load the order sessions\n",
    "sessions_df = pl.read_parquet(OUTPUT_DIR / 'order_sessions.parquet')\n",
    "\n",
    "# Group items by session\n",
    "session_items = sessions_df.group_by('session_id').agg(pl.col('item_id').alias('items'))\n",
    "\n",
    "print(f\"Total sessions with orders: {len(session_items):,}\")\n",
    "\n",
    "# Build co-occurrence pairs\n",
    "# For each session, create all pairs of items that appeared together\n",
    "print(\"Generating item pairs from sessions...\")\n",
    "\n",
    "# Generate all unique pairs from a list of items\n",
    "def generate_pairs(items):\n",
    "    pairs = []\n",
    "    items_list = sorted(set(items))  # Remove duplicates within session\n",
    "    for i in range(len(items_list)):\n",
    "        for j in range(i + 1, len(items_list)):\n",
    "            pairs.append((items_list[i], items_list[j]))\n",
    "    return pairs\n",
    "\n",
    "# This is memory intensive, so we'll process in chunks\n",
    "chunk_size = 50000\n",
    "all_pairs = []\n",
    "\n",
    "for i in range(0, len(session_items), chunk_size):\n",
    "    chunk = session_items[i:i + chunk_size]\n",
    "    \n",
    "    for row in chunk.iter_rows(named=True):\n",
    "        items = row['items']\n",
    "        if len(items) > 1:  # Need at least 2 items for a pair\n",
    "            pairs = generate_pairs(items)\n",
    "            all_pairs.extend(pairs)\n",
    "    \n",
    "    if (i // chunk_size + 1) % 10 == 0:\n",
    "        print(f\"Processed {i + chunk_size:,} sessions...\")\n",
    "\n",
    "print(f\"\\nTotal item pairs generated: {len(all_pairs):,}\")\n",
    "\n",
    "# Count co-occurrences\n",
    "print(\"Counting co-occurrence frequencies...\")\n",
    "pair_counts = Counter(all_pairs)\n",
    "\n",
    "# Convert to dataframe\n",
    "cooccurrence_df = pl.DataFrame({'item_a': [pair[0] for pair in pair_counts.keys()],\n",
    "                                'item_b': [pair[1] for pair in pair_counts.keys()],\n",
    "                                'cooccurrence_count': list(pair_counts.values())\n",
    "                            }).sort('cooccurrence_count', descending=True)\n",
    "\n",
    "print(f\"\\nUnique item pairs with co-occurrences: {len(cooccurrence_df):,}\")\n",
    "print(\"\\nTop 10 Most Frequently Bought Together:\")\n",
    "print(cooccurrence_df.head(10))\n",
    "\n",
    "# Save co-occurrence matrix\n",
    "cooccurrence_path = MODELS_DIR / 'item_cooccurrence.parquet'\n",
    "cooccurrence_df.write_parquet(cooccurrence_path)\n",
    "print(f\"\\nCo-occurrence matrix saved to {cooccurrence_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd505aea-e131-4256-b190-5502a5440087",
   "metadata": {},
   "source": [
    "### FreBoTo API Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbb2244f-a50b-4d92-98dd-0f8c14f095a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FreBoTo recommendation API function...\n",
      "Testing FreBoTo for item 125278:\n",
      "  Item 1629608: bought together 671 times\n",
      "  Item 162064: bought together 486 times\n",
      "  Item 1787141: bought together 346 times\n",
      "  Item 569212: bought together 202 times\n",
      "  Item 16151: bought together 165 times\n",
      "\n",
      "FreBoTo model ready for deployment\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating FreBoTo recommendation API function...\")\n",
    "\n",
    "# Load co-occurrence matrix\n",
    "cooccurrence_data = pl.read_parquet(MODELS_DIR / 'item_cooccurrence.parquet')\n",
    "\n",
    "# This function gets the item id and return the top_n co-occurrence items for it\n",
    "def get_frequently_bought_together(item_id, top_n=5):\n",
    "\n",
    "    # Find all pairs containing this item\n",
    "    related_items = cooccurrence_data.filter((pl.col('item_a') == item_id) | (pl.col('item_b') == item_id)\n",
    "                                                ).with_columns([\n",
    "                                                    # Get the other item in the pair\n",
    "                                                    pl.when(pl.col('item_a') == item_id)\n",
    "                                                      .then(pl.col('item_b'))\n",
    "                                                      .otherwise(pl.col('item_a'))\n",
    "                                                      .alias('recommended_item')\n",
    "                                                ]).sort('cooccurrence_count', descending=True).head(top_n)\n",
    "    \n",
    "    return related_items.select(['recommended_item', 'cooccurrence_count']).to_dicts()\n",
    "\n",
    "# Test the function with a popular item\n",
    "test_item = cooccurrence_data[0, 'item_a']\n",
    "print(f\"Testing FreBoTo for item {test_item}:\")\n",
    "recommendations = get_frequently_bought_together(test_item, top_n=5)\n",
    "for rec in recommendations:\n",
    "    print(f\"  Item {rec['recommended_item']}: bought together {rec['cooccurrence_count']} times\")\n",
    "\n",
    "# Save model metadata\n",
    "freboto_metadata = {\n",
    "    'model_name': 'frequently_bought_together',\n",
    "    'model_version': '1.0',\n",
    "    'created_at': pd.Timestamp.now().isoformat(),\n",
    "    'total_pairs': len(cooccurrence_data),\n",
    "    'min_cooccurrence': int(cooccurrence_data['cooccurrence_count'].min()),\n",
    "    'max_cooccurrence': int(cooccurrence_data['cooccurrence_count'].max()),\n",
    "    'description': 'Item co-occurrence from purchase sessions'\n",
    "}\n",
    "\n",
    "with open(MODELS_DIR / 'freboto_metadata.json', 'w') as f:\n",
    "    json.dump(freboto_metadata, f, indent=2)\n",
    "\n",
    "print(\"\\nFreBoTo model ready for deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd9d442-22fd-4725-b947-1c65904cacbf",
   "metadata": {},
   "source": [
    "## Real-time recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561bbc9-f016-49e4-abc9-8d43bbf0057a",
   "metadata": {},
   "source": [
    "### Collaborative Filtering - User-Item Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8a6bd5f-0cc3-4452-baa4-fe6e7fe57620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Collaborative Filtering Model\n",
      "This model enables personalized real-time recommendations\n",
      "Creating user-item interaction matrix...\n",
      "Working with 500,000 interactions\n",
      "Unique sessions: 7633\n",
      "Unique items (before filtering): 160980\n",
      "Items with at least 5 interactions: 8504\n",
      "Final unique items (limited to top 5000): 5000\n",
      "Final unique sessions: 6024\n",
      "Final interactions: 61,367\n",
      "\n",
      "Data types after processing:\n",
      "[Int64, Int64, Int32]\n",
      "\n",
      "User-item interactions saved to user_item_interactions.parquet\n",
      "\n",
      "Note: Limited to top 5000 items to keep similarity matrix manageable\n",
      "In production, use matrix factorization (ALS) or approximate methods\n"
     ]
    }
   ],
   "source": [
    "print(\"Building Collaborative Filtering Model\")\n",
    "print(\"This model enables personalized real-time recommendations\")\n",
    "\n",
    "# For collaborative filtering, we need a user-item interaction matrix\n",
    "# We'll use implicit feedback (any interaction = positive signal)\n",
    "\n",
    "print(\"Creating user-item interaction matrix...\")\n",
    "\n",
    "# Sample interactions for a manageable matrix size\n",
    "sample_size = 500000\n",
    "sampled_interactions = df_final.head(sample_size).collect()\n",
    "\n",
    "print(f\"Working with {len(sampled_interactions):,} interactions\")\n",
    "\n",
    "# Create interaction scores based on event type\n",
    "sampled_interactions = sampled_interactions.with_columns([\n",
    "    pl.when(pl.col('event_type') == 'clicks').then(pl.lit(1))\n",
    "      .when(pl.col('event_type') == 'carts').then(pl.lit(2))\n",
    "      .when(pl.col('event_type') == 'orders').then(pl.lit(3))\n",
    "      .otherwise(pl.lit(1))\n",
    "      .cast(pl.Int32)\n",
    "      .alias('interaction_score')\n",
    "])\n",
    "\n",
    "# Aggregate by session and item\n",
    "user_item_scores = sampled_interactions.group_by(['session_id', 'item_id']).agg([\n",
    "    pl.col('interaction_score').sum().alias('total_score')\n",
    "])\n",
    "\n",
    "print(f\"Unique sessions: {user_item_scores['session_id'].n_unique()}\")\n",
    "print(f\"Unique items (before filtering): {user_item_scores['item_id'].n_unique()}\")\n",
    "\n",
    "# CRITICAL: Filter to only keep popular items to reduce matrix size\n",
    "# Keep only items with at least N interactions\n",
    "min_interactions = 5\n",
    "item_interaction_counts = user_item_scores.group_by('item_id').agg(pl.len().alias('interaction_count'))\n",
    "\n",
    "popular_items = item_interaction_counts.filter(pl.col('interaction_count') >= min_interactions)\n",
    "\n",
    "print(f\"Items with at least {min_interactions} interactions: {len(popular_items)}\")\n",
    "\n",
    "# Keep only interactions with popular items\n",
    "user_item_scores_filtered = user_item_scores.join(popular_items.select('item_id'), on='item_id', how='inner')\n",
    "\n",
    "# Further limit: take only top N most popular items\n",
    "max_items = 5000  # Limit to 5000 items for memory efficiency\n",
    "top_items = item_interaction_counts.sort('interaction_count', descending=True).head(max_items)\n",
    "\n",
    "user_item_scores_filtered = user_item_scores_filtered.join(top_items.select('item_id'), on='item_id', how='inner')\n",
    "\n",
    "print(f\"Final unique items (limited to top {max_items}): {user_item_scores_filtered['item_id'].n_unique()}\")\n",
    "print(f\"Final unique sessions: {user_item_scores_filtered['session_id'].n_unique()}\")\n",
    "print(f\"Final interactions: {len(user_item_scores_filtered):,}\")\n",
    "\n",
    "# Ensure total_score is numeric\n",
    "user_item_scores_filtered = user_item_scores_filtered.with_columns([pl.col('total_score').cast(pl.Int32)])\n",
    "\n",
    "print(f\"\\nData types after processing:\")\n",
    "print(user_item_scores_filtered.dtypes)\n",
    "\n",
    "# Save for matrix construction\n",
    "user_item_path = OUTPUT_DIR / 'user_item_interactions.parquet'\n",
    "user_item_scores_filtered.write_parquet(user_item_path)\n",
    "\n",
    "print(f\"\\nUser-item interactions saved to {user_item_path.name}\")\n",
    "print(\"\\nNote: Limited to top 5000 items to keep similarity matrix manageable\")\n",
    "print(\"In production, use matrix factorization (ALS) or approximate methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f827e5-4e68-4f7a-b860-4914477767cd",
   "metadata": {},
   "source": [
    "### Sparse Matrix Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7dc0386-61f4-40bc-b45e-2e43f196bb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing item-item similarity matrix...\n",
      "Data loaded: 61,367 interactions\n",
      "Columns: ['session_id', 'item_id', 'total_score']\n",
      "Data types:\n",
      "session_id     int64\n",
      "item_id        int64\n",
      "total_score    int32\n",
      "dtype: object\n",
      "\n",
      "Creating user-item matrix...\n",
      "Matrix shape: (6024, 5000)\n",
      "\n",
      "Converting to sparse matrix...\n",
      "Sparse matrix shape: (6024, 5000)\n",
      "Non-zero entries: 61,367\n",
      "Sparsity: 99.80%\n",
      "\n",
      "Computing item-item cosine similarity...\n",
      "Similarity matrix shape: (5000, 5000)\n",
      "\n",
      "Item similarity model saved to item_similarity_model.pkl\n",
      "Total items in similarity matrix: 5,000\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing item-item similarity matrix...\")\n",
    "\n",
    "# Load user-item interactions\n",
    "interactions_df = pl.read_parquet(OUTPUT_DIR / 'user_item_interactions.parquet')\n",
    "\n",
    "# Convert to pandas for sklearn compatibility\n",
    "interactions_pd = interactions_df.to_pandas()\n",
    "\n",
    "print(f\"Data loaded: {len(interactions_pd):,} interactions\")\n",
    "print(f\"Columns: {interactions_pd.columns.tolist()}\")\n",
    "print(f\"Data types:\\n{interactions_pd.dtypes}\")\n",
    "\n",
    "# Create a pivot table (sparse matrix)\n",
    "print(\"\\nCreating user-item matrix...\")\n",
    "user_item_matrix = interactions_pd.pivot_table(index='session_id', columns='item_id', values='total_score',\n",
    "                                                    aggfunc='sum',  # Explicit aggregation function\n",
    "                                                    fill_value=0\n",
    "                                                )\n",
    "\n",
    "print(f\"Matrix shape: {user_item_matrix.shape}\")\n",
    "\n",
    "# Convert to sparse matrix for memory efficiency\n",
    "print(\"\\nConverting to sparse matrix...\")\n",
    "sparse_matrix = csr_matrix(user_item_matrix.values)\n",
    "print(f\"Sparse matrix shape: {sparse_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {sparse_matrix.nnz:,}\")\n",
    "print(f\"Sparsity: {(1 - sparse_matrix.nnz / (sparse_matrix.shape[0] * sparse_matrix.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nComputing item-item cosine similarity...\")\n",
    "# Transpose to get item-item similarity (instead of user-user)\n",
    "item_similarity = cosine_similarity(sparse_matrix.T)\n",
    "\n",
    "print(f\"Similarity matrix shape: {item_similarity.shape}\")\n",
    "\n",
    "# Save the similarity matrix and item mapping\n",
    "item_ids = user_item_matrix.columns.tolist()\n",
    "\n",
    "similarity_data = {\n",
    "    'similarity_matrix': item_similarity,\n",
    "    'item_ids': item_ids,\n",
    "    'item_to_idx': {item: idx for idx, item in enumerate(item_ids)}\n",
    "}\n",
    "\n",
    "joblib.dump(similarity_data, MODELS_DIR / 'item_similarity_model.pkl')\n",
    "print(f\"\\nItem similarity model saved to item_similarity_model.pkl\")\n",
    "print(f\"Total items in similarity matrix: {len(item_ids):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f00f2f-e0d3-4012-bc58-8c898692a83b",
   "metadata": {},
   "source": [
    "### Real-Time Recommendation API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "270e77c6-0175-4068-833c-4da294fde775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Real-Time Recommendation API \n",
      "Test user history: [890, 900, 1050]\n",
      "\n",
      "Real-time recommendations:\n",
      "  1. Item 725605 (score: 0.243)\n",
      "  2. Item 450455 (score: 0.223)\n",
      "  3. Item 177210 (score: 0.212)\n",
      "  4. Item 1812305 (score: 0.173)\n",
      "  5. Item 715263 (score: 0.167)\n",
      "\n",
      "Real-time recommendation API ready\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Real-Time Recommendation API \")\n",
    "\n",
    "# Load models\n",
    "similarity_data = joblib.load(MODELS_DIR / 'item_similarity_model.pkl')\n",
    "similarity_matrix = similarity_data['similarity_matrix']\n",
    "item_ids = similarity_data['item_ids']\n",
    "item_to_idx = similarity_data['item_to_idx']\n",
    "\n",
    "# This function gent a user's recent interactions and returns list of top_n recommended item IDs with scores\n",
    "def get_realtime_recommendations(user_history, top_n=10):\n",
    "    \n",
    "    if not user_history:\n",
    "        # Fallback to top sellers if no history\n",
    "        return get_top_sellers(top_n)\n",
    "    \n",
    "    # Get indices of items in user history\n",
    "    valid_items = [item for item in user_history if item in item_to_idx]\n",
    "    \n",
    "    if not valid_items:\n",
    "        return get_top_sellers(top_n)\n",
    "    \n",
    "    item_indices = [item_to_idx[item] for item in valid_items]\n",
    "    \n",
    "    # Average similarity scores across all items in history\n",
    "    similarity_scores = similarity_matrix[item_indices].mean(axis=0)\n",
    "    \n",
    "    # Remove items already in history\n",
    "    for idx in item_indices:\n",
    "        similarity_scores[idx] = -1\n",
    "    \n",
    "    # Get top N items\n",
    "    top_indices = np.argsort(similarity_scores)[::-1][:top_n]\n",
    "    \n",
    "    recommendations = [\n",
    "        {\n",
    "            'item_id': item_ids[idx],\n",
    "            'score': float(similarity_scores[idx])\n",
    "        }\n",
    "        for idx in top_indices\n",
    "    ]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test the real-time recommendation system\n",
    "test_history = item_ids[:3]  # Simulate user viewing 3 items\n",
    "print(f\"Test user history: {test_history}\")\n",
    "print(\"\\nReal-time recommendations:\")\n",
    "recs = get_realtime_recommendations(test_history, top_n=5)\n",
    "for i, rec in enumerate(recs, 1):\n",
    "    print(f\"  {i}. Item {rec['item_id']} (score: {rec['score']:.3f})\")\n",
    "\n",
    "# Save API metadata\n",
    "realtime_metadata = {\n",
    "    'model_name': 'realtime_collaborative_filtering',\n",
    "    'model_version': '1.0',\n",
    "    'created_at': pd.Timestamp.now().isoformat(),\n",
    "    'total_items': len(item_ids),\n",
    "    'description': 'Real-time personalized recommendations using collaborative filtering'\n",
    "}\n",
    "\n",
    "with open(MODELS_DIR / 'realtime_model_metadata.json', 'w') as f:\n",
    "    json.dump(realtime_metadata, f, indent=2)\n",
    "\n",
    "print(\"\\nReal-time recommendation API ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0cee8f-6bda-4b23-8296-fbbc1db33f19",
   "metadata": {},
   "source": [
    "### API Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ff4f4e0-7705-40a9-977d-ffd9d01585dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API endpoints defined. Run with: uvicorn api:app --reload\n"
     ]
    }
   ],
   "source": [
    "# Below is a simple FastAPI example showing how the recommendation system could be exposed as an API for the customer's application.\n",
    "app = FastAPI()\n",
    "\n",
    "class RecommendRequest(BaseModel):\n",
    "    user_history: List[int]\n",
    "    n_recommendations: int = 10\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health_check():\n",
    "    \"\"\"Check if API is running\"\"\"\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "@app.get(\"/recommendations/top-sellers\")\n",
    "def get_top_sellers(n: int = 10):\n",
    "    \"\"\"Get top selling products\"\"\"\n",
    "    return {\"items\": api.get_top_sellers(n)}\n",
    "\n",
    "@app.get(\"/recommendations/frequently-bought-together/{item_id}\")\n",
    "def get_frequently_bought_together(item_id: int, n: int = 5):\n",
    "    \"\"\"Get items frequently bought with this product\"\"\"\n",
    "    result = api.get_frequently_bought_together(item_id, n)\n",
    "    return {\"items\": result}\n",
    "\n",
    "@app.post(\"/recommendations/personalized\")\n",
    "def get_personalized(request: RecommendRequest):\n",
    "    \"\"\"Get personalized recommendations based on user history\"\"\"\n",
    "    result = api.get_personalized_recommendations(\n",
    "        request.user_history, \n",
    "        request.n_recommendations\n",
    "    )\n",
    "    return {\"items\": result}\n",
    "\n",
    "print(\"API endpoints defined. Run with: uvicorn api:app --reload\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3ed272-0d51-407f-ae7b-3166a4d7af47",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a5cdc20-b789-41ac-915e-08989770c9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recommendation models\n",
      "\n",
      "Catalog Overview:\n",
      "  Total items: 1,855,603\n",
      "  Active items (10+ interactions): 1,389,510 (74.9%)\n",
      "  Items with orders: 657,940 (35.5%)\n",
      "\n",
      "1. Top Sellers Model:\n",
      "  Recommendable items: 100\n",
      "  Avg orders per item: 1643\n",
      "  Min orders in top 100: 1095\n",
      "\n",
      "2. FreBoTo Model:\n",
      "  Coverage of active items: 18.1%\n",
      "  Total item pairs: 9,564,041\n",
      "  Unique items with associations: 251,901\n",
      "  Median co-occurrence: 1\n",
      "\n",
      "3. Real-Time CF Model:\n",
      "  Coverage of active items: 0.4%\n",
      "  Items in similarity matrix: 5,000\n",
      "  Recommendation diversity: 100.0%\n",
      "  Avg similarity score: 0.093\n",
      "\n",
      "Business Impact:\n",
      "  Top 100 items represent 3.2% of total orders\n",
      "\n",
      "Evaluation metrics saved\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating recommendation models\")\n",
    "\n",
    "# Load necessary data\n",
    "top_sellers_data = pl.read_parquet(MODELS_DIR / 'top_sellers_purchase.parquet')\n",
    "freboto_data = pl.read_parquet(MODELS_DIR / 'item_cooccurrence.parquet')\n",
    "item_stats = pl.read_parquet(OUTPUT_DIR / 'item_popularity_stats.parquet')\n",
    "\n",
    "# Calculate catalog statistics\n",
    "total_items = item_stats.height\n",
    "active_items = item_stats.filter(pl.col('total_interactions') >= 10).height\n",
    "items_with_orders = item_stats.filter(pl.col('orders') > 0).height\n",
    "\n",
    "print(f\"\\nCatalog Overview:\")\n",
    "print(f\"  Total items: {total_items:,}\")\n",
    "print(f\"  Active items (10+ interactions): {active_items:,} ({active_items/total_items*100:.1f}%)\")\n",
    "print(f\"  Items with orders: {items_with_orders:,} ({items_with_orders/total_items*100:.1f}%)\")\n",
    "\n",
    "# Evaluate Top Sellers\n",
    "print(f\"\\n1. Top Sellers Model:\")\n",
    "print(f\"  Recommendable items: {len(top_sellers_data)}\")\n",
    "print(f\"  Avg orders per item: {top_sellers_data['orders'].mean():.0f}\")\n",
    "print(f\"  Min orders in top 100: {top_sellers_data['orders'].min()}\")\n",
    "\n",
    "# Evaluate FreBoTo\n",
    "unique_items_freboto = len(set(freboto_data['item_a'].to_list() + freboto_data['item_b'].to_list()))\n",
    "freboto_coverage = unique_items_freboto / active_items * 100\n",
    "\n",
    "print(f\"\\n2. FreBoTo Model:\")\n",
    "print(f\"  Coverage of active items: {freboto_coverage:.1f}%\")\n",
    "print(f\"  Total item pairs: {len(freboto_data):,}\")\n",
    "print(f\"  Unique items with associations: {unique_items_freboto:,}\")\n",
    "print(f\"  Median co-occurrence: {freboto_data['cooccurrence_count'].median():.0f}\")\n",
    "\n",
    "# Evaluate Real-Time CF\n",
    "cf_coverage = len(item_ids) / active_items * 100\n",
    "\n",
    "print(f\"\\n3. Real-Time CF Model:\")\n",
    "print(f\"  Coverage of active items: {cf_coverage:.1f}%\")\n",
    "print(f\"  Items in similarity matrix: {len(item_ids):,}\")\n",
    "\n",
    "# Test recommendation diversity\n",
    "test_user = item_ids[:5]\n",
    "recs = get_realtime_recommendations(test_user, top_n=20)\n",
    "rec_items = [r['item_id'] for r in recs]\n",
    "diversity = len(set(rec_items)) / len(rec_items) * 100\n",
    "rec_scores = [r['score'] for r in recs]\n",
    "\n",
    "print(f\"  Recommendation diversity: {diversity:.1f}%\")\n",
    "print(f\"  Avg similarity score: {np.mean(rec_scores):.3f}\")\n",
    "\n",
    "# Business impact\n",
    "top_100_revenue = top_sellers_data['orders'].sum() / item_stats['orders'].sum() * 100\n",
    "\n",
    "print(f\"\\nBusiness Impact:\")\n",
    "print(f\"  Top 100 items represent {top_100_revenue:.1f}% of total orders\")\n",
    "\n",
    "# Save metrics\n",
    "evaluation_metrics = {\n",
    "    'catalog_size': int(total_items),\n",
    "    'active_items': int(active_items),\n",
    "    'top_sellers_avg_orders': float(top_sellers_data['orders'].mean()),\n",
    "    'freboto_coverage_pct': float(freboto_coverage),\n",
    "    'cf_coverage_pct': float(cf_coverage),\n",
    "    'top_100_revenue_pct': float(top_100_revenue),\n",
    "    'evaluation_date': pd.Timestamp.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'evaluation_metrics.json', 'w') as f:\n",
    "    json.dump(evaluation_metrics, f, indent=2)\n",
    "\n",
    "print(\"\\nEvaluation metrics saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef6b07-9612-49ac-a938-490733e8e9cb",
   "metadata": {},
   "source": [
    "## Unified API Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "610f4608-a13a-409a-9334-a24eb87566fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Unified Recommendation API\n",
      "All models loaded successfully\n",
      "\n",
      "API Test Cases:\n",
      "\n",
      "1. Homepage recommendations:\n",
      "   Type: top_sellers\n",
      "   Items: [231487, 166037, 1733943]\n",
      "\n",
      "2. Product page recommendations:\n",
      "   Type: frequently_bought_together\n",
      "   For item: 890\n",
      "   Recommended: {'recommended_item': 937825, 'cooccurrence_count': 5}\n",
      "\n",
      "3. Personalized recommendations:\n",
      "   Type: personalized\n",
      "   User history: [900, 1050, 1089]\n",
      "   Recommended: [177210, 1771635, 405140]\n",
      "\n",
      "API ready for deployment\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating Unified Recommendation API\")\n",
    "\n",
    "# This is a unified API for all recommendation models\n",
    "class RecommendationAPI:\n",
    "    # First we load all models on initialization\n",
    "    def __init__(self, models_dir):\n",
    "        self.models_dir = Path(models_dir)\n",
    "        \n",
    "        # Load top sellers\n",
    "        self.top_sellers = pl.read_parquet(self.models_dir / 'top_sellers_purchase.parquet')\n",
    "        \n",
    "        # Load FreBoTo\n",
    "        self.cooccurrence = pl.read_parquet(self.models_dir / 'item_cooccurrence.parquet')\n",
    "        \n",
    "        # Load CF model\n",
    "        cf_data = joblib.load(self.models_dir / 'item_similarity_model.pkl')\n",
    "        self.similarity_matrix = cf_data['similarity_matrix']\n",
    "        self.item_ids = cf_data['item_ids']\n",
    "        self.item_to_idx = cf_data['item_to_idx']\n",
    "        \n",
    "        print(\"All models loaded successfully\")\n",
    "    \n",
    "    # This function return the top selling products\n",
    "    def get_top_sellers(self, n=10):\n",
    "        return self.top_sellers.head(n)['item_id'].to_list()\n",
    "    \n",
    "    # This return the top_n items frequently bought for a given item\n",
    "    def get_frequently_bought_together(self, item_id, top_n=5):\n",
    "        related = self.cooccurrence.filter(\n",
    "            (pl.col('item_a') == item_id) | (pl.col('item_b') == item_id)\n",
    "        ).with_columns([\n",
    "            pl.when(pl.col('item_a') == item_id)\n",
    "              .then(pl.col('item_b'))\n",
    "              .otherwise(pl.col('item_a'))\n",
    "              .alias('recommended_item')\n",
    "        ]).sort('cooccurrence_count', descending=True).head(top_n)\n",
    "        \n",
    "        return related.select(['recommended_item', 'cooccurrence_count']).to_dicts()\n",
    "    \n",
    "    # This function returns personalized recommendations based on user history\n",
    "    def get_personalized_recommendations(self, user_history, top_n=10):\n",
    "        \n",
    "        # If there is no history it just recommend the top sellers\n",
    "        if not user_history:\n",
    "            return self.get_top_sellers(top_n)\n",
    "        \n",
    "        valid_items = [item for item in user_history if item in self.item_to_idx]\n",
    "        \n",
    "        if not valid_items:\n",
    "            return self.get_top_sellers(top_n)\n",
    "        \n",
    "        item_indices = [self.item_to_idx[item] for item in valid_items]\n",
    "        similarity_scores = self.similarity_matrix[item_indices].mean(axis=0)\n",
    "        \n",
    "        for idx in item_indices:\n",
    "            similarity_scores[idx] = -1\n",
    "        \n",
    "        top_indices = np.argsort(similarity_scores)[::-1][:top_n]\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                'item_id': self.item_ids[idx],\n",
    "                'score': float(similarity_scores[idx])\n",
    "            }\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "    \n",
    "    # We can also implement a context-aware recommendations based on page type (home page-top seller, product page- FreBoTo items, cart - user recom)\n",
    "    def get_recommendations(self, page_type, item_id=None, user_history=None, top_n=10):\n",
    "        if page_type == 'homepage':\n",
    "            items = self.get_top_sellers(top_n)\n",
    "            return {\n",
    "                'recommendation_type': 'top_sellers',\n",
    "                'items': items,\n",
    "                'reason': 'Most popular products'\n",
    "            }\n",
    "        \n",
    "        elif page_type == 'product' and item_id:\n",
    "            items = self.get_frequently_bought_together(item_id, top_n)\n",
    "            return {\n",
    "                'recommendation_type': 'frequently_bought_together',\n",
    "                'items': items,\n",
    "                'reason': 'Customers who bought this also bought'\n",
    "            }\n",
    "        \n",
    "        elif page_type == 'cart' or user_history:\n",
    "            items = self.get_personalized_recommendations(user_history or [], top_n)\n",
    "            return {\n",
    "                'recommendation_type': 'personalized',\n",
    "                'items': items,\n",
    "                'reason': 'Based on your browsing history'\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            # Fallback to top sellers\n",
    "            items = self.get_top_sellers(top_n)\n",
    "            return {\n",
    "                'recommendation_type': 'top_sellers',\n",
    "                'items': items,\n",
    "                'reason': 'Most popular products'\n",
    "            }\n",
    "\n",
    "# Initialize API\n",
    "api = RecommendationAPI(MODELS_DIR)\n",
    "\n",
    "# Test different scenarios\n",
    "print(\"\\nAPI Test Cases:\\n\")\n",
    "\n",
    "print(\"1. Homepage recommendations:\")\n",
    "homepage_recs = api.get_recommendations('homepage', top_n=5)\n",
    "print(f\"   Type: {homepage_recs['recommendation_type']}\")\n",
    "print(f\"   Items: {homepage_recs['items'][:3]}\")\n",
    "\n",
    "print(\"\\n2. Product page recommendations:\")\n",
    "test_item = item_ids[0]\n",
    "product_recs = api.get_recommendations('product', item_id=test_item, top_n=5)\n",
    "print(f\"   Type: {product_recs['recommendation_type']}\")\n",
    "print(f\"   For item: {test_item}\")\n",
    "if product_recs['items']:\n",
    "    print(f\"   Recommended: {product_recs['items'][0]}\")\n",
    "\n",
    "print(\"\\n3. Personalized recommendations:\")\n",
    "test_history = item_ids[1:4]\n",
    "personal_recs = api.get_recommendations('cart', user_history=test_history, top_n=5)\n",
    "print(f\"   Type: {personal_recs['recommendation_type']}\")\n",
    "print(f\"   User history: {test_history}\")\n",
    "print(f\"   Recommended: {[r['item_id'] for r in personal_recs['items'][:3]]}\")\n",
    "\n",
    "print(\"\\nAPI ready for deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63c0ab-de3b-4e20-bc10-dc813c54759e",
   "metadata": {},
   "source": [
    "# Deployment architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bf5b9c-cdda-441f-bb5e-6c4200b77228",
   "metadata": {},
   "source": [
    "## Deployment Architecture\n",
    "\n",
    "Now that the recommender models are trained and evaluated, the next step is to design how this system could be deployed to serve real-time recommendations to end users.\n",
    "\n",
    "Because this is a feasibility study, the deployment plan focuses on a **prototype-level cloud-agnostic architecture** that can later scale with minimal changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd8d2c-eb38-431a-9f26-66e2c0b867f1",
   "metadata": {},
   "source": [
    "### System Overview\n",
    "\n",
    "The deployment setup consists of four main components:\n",
    "\n",
    "1. **Data & Model Storage** – historical user–item interactions and trained model artifacts stored in a secure bucket or database.\n",
    "2. **Model Service Layer** – a lightweight API (e.g., FastAPI) hosting the trained recommendation model for online predictions.\n",
    "3. **Application Layer** – the e-commerce platform or front-end system that queries the recommendation API.\n",
    "4. **Monitoring & Logging Layer** – optional service for tracking API performance and user feedback for model updates.\n",
    "\n",
    "Below is a simple architecture diagram illustrating how the pieces fit together:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4cf939d-9050-4606-b201-c0bc1ffd8837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 14.0.2 (20251019.1705)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"338pt\" height=\"279pt\"\n",
       " viewBox=\"0.00 0.00 338.00 279.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 274.5)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-274.5 333.86,-274.5 333.86,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M207.25,-270.5C207.25,-270.5 128.25,-270.5 128.25,-270.5 122.25,-270.5 116.25,-264.5 116.25,-258.5 116.25,-258.5 116.25,-246.5 116.25,-246.5 116.25,-240.5 122.25,-234.5 128.25,-234.5 128.25,-234.5 207.25,-234.5 207.25,-234.5 213.25,-234.5 219.25,-240.5 219.25,-246.5 219.25,-246.5 219.25,-258.5 219.25,-258.5 219.25,-264.5 213.25,-270.5 207.25,-270.5\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"167.75\" y=\"-247.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">User / Web App</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M245.88,-197.5C245.88,-197.5 89.62,-197.5 89.62,-197.5 83.62,-197.5 77.62,-191.5 77.62,-185.5 77.62,-185.5 77.62,-173.5 77.62,-173.5 77.62,-167.5 83.62,-161.5 89.62,-161.5 89.62,-161.5 245.88,-161.5 245.88,-161.5 251.88,-161.5 257.88,-167.5 257.88,-173.5 257.88,-173.5 257.88,-185.5 257.88,-185.5 257.88,-191.5 251.88,-197.5 245.88,-197.5\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"167.75\" y=\"-174.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">API Gateway / Load Balancer</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.75,-234.31C167.75,-226.73 167.75,-217.6 167.75,-209.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.25,-209.04 167.75,-199.04 164.25,-209.04 171.25,-209.04\"/>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>C</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M252.62,-124.5C252.62,-124.5 82.88,-124.5 82.88,-124.5 76.88,-124.5 70.88,-118.5 70.88,-112.5 70.88,-112.5 70.88,-100.5 70.88,-100.5 70.88,-94.5 76.88,-88.5 82.88,-88.5 82.88,-88.5 252.62,-88.5 252.62,-88.5 258.62,-88.5 264.62,-94.5 264.62,-100.5 264.62,-100.5 264.62,-112.5 264.62,-112.5 264.62,-118.5 258.62,-124.5 252.62,-124.5\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"167.75\" y=\"-101.45\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Model Service (FastAPI / Flask)</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.75,-161.31C167.75,-153.73 167.75,-144.6 167.75,-136.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"171.25,-136.04 167.75,-126.04 164.25,-136.04 171.25,-136.04\"/>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>D</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M149.5,-36C149.5,-36 12,-36 12,-36 6,-36 0,-30 0,-24 0,-24 0,-12 0,-12 0,-6 6,0 12,0 12,0 149.5,0 149.5,0 155.5,0 161.5,-6 161.5,-12 161.5,-12 161.5,-24 161.5,-24 161.5,-30 155.5,-36 149.5,-36\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"80.75\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Model Registry &amp; Storage</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;D -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>C&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M101.46,-88.12C93.84,-83.55 87.02,-77.78 82,-70.5 77.43,-63.87 75.92,-55.54 75.88,-47.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79.36,-47.95 76.59,-37.73 72.38,-47.45 79.36,-47.95\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"157.38\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Load model / store artifacts</text>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>E</title>\n",
       "<path fill=\"lightgrey\" stroke=\"lightgrey\" d=\"M301,-36C301,-36 208.5,-36 208.5,-36 202.5,-36 196.5,-30 196.5,-24 196.5,-24 196.5,-12 196.5,-12 196.5,-6 202.5,0 208.5,0 208.5,0 301,0 301,0 307,0 313,-6 313,-12 313,-12 313,-24 313,-24 313,-30 307,-36 301,-36\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"254.75\" y=\"-12.95\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Logs / Monitoring</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;E -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>C&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M210.23,-88C218.44,-83.22 226.44,-77.41 232.75,-70.5 238.79,-63.89 243.33,-55.36 246.67,-47.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"249.95,-48.41 250,-37.81 243.35,-46.06 249.95,-48.41\"/>\n",
       "<text xml:space=\"preserve\" text-anchor=\"middle\" x=\"286.73\" y=\"-57.2\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Logs &amp; metrics</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x11bd1dc0ec0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = Digraph(comment=\"Recommendation System Deployment\", format=\"png\")\n",
    "dot.attr('node', shape='box', style='rounded,filled', color='lightgrey')\n",
    "\n",
    "# Nodes\n",
    "dot.node('A', 'User / Web App')\n",
    "dot.node('B', 'API Gateway / Load Balancer')\n",
    "dot.node('C', 'Model Service (FastAPI / Flask)')\n",
    "dot.node('D', 'Model Registry & Storage')\n",
    "dot.node('E', 'Logs / Monitoring')\n",
    "\n",
    "# Edges\n",
    "dot.edge('A', 'B')\n",
    "dot.edge('B', 'C')\n",
    "dot.edge('C', 'D', label='Load model / store artifacts',  fontsize=\"14pt\")\n",
    "dot.edge('C', 'E', label='Logs & metrics',  fontsize=\"14pt\")\n",
    "\n",
    "dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f3710-ddd9-46f0-a71a-4920be9cc8ed",
   "metadata": {},
   "source": [
    "## Deployment Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475b281-a0aa-4e2c-8576-0ef86c28f9f5",
   "metadata": {},
   "source": [
    "### Deployment Flow Summary\n",
    "\n",
    "1. **Model Packaging:** The trained recommender is serialized using `pickle` or `joblib`.\n",
    "2. **Service Startup:** The API server loads the latest model artifact on startup.\n",
    "3. **Prediction Request:** When a user visits the site, the frontend sends a request with their session or product history.\n",
    "4. **Model Inference:** The model service computes and returns the top-N product recommendations.\n",
    "5. **Feedback Logging:** User interactions (clicks, purchases) are logged for future retraining.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843b91b-ff28-47b9-a6b2-81ebcd168c92",
   "metadata": {},
   "source": [
    "## Model Versioning & Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cda066-6e18-4bbf-b8b8-5f572f4ef3b7",
   "metadata": {},
   "source": [
    "## Model Versioning and Tracking\n",
    "\n",
    "For this prototype, we can maintain a **simple versioning scheme** by saving trained model artifacts with version tags (e.g., `recommender_v1.pkl`, `recommender_v2.pkl`).\n",
    "\n",
    "In a production setup, this could be extended using:\n",
    "- **MLflow** or **Weights & Biases** for experiment tracking\n",
    "- A **model registry** for controlled rollout of new versions\n",
    "- **Scheduled retraining** triggered by new data\n",
    "\n",
    "For this case study, model versioning is mentioned conceptually but I did not implement it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dfed2f-4682-415a-9608-badc869b2054",
   "metadata": {},
   "source": [
    "# Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055733ca-952e-4850-b33c-50d2b5f7b3a0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook presented a full end-to-end prototype for a recommendation system, including:\n",
    "- Data preparation and exploratory analysis\n",
    "- Model training and evaluation\n",
    "- A deployable design for real-time recommendation\n",
    "\n",
    "The architecture and API examples are kept conceptual and interpretable to demonstrate feasibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63fed8b-8b2e-4a7d-9328-071e16fdbda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be0b53d-3b02-4b4c-8427-52296619b70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
